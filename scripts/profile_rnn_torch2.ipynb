{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import pypose as pp\n",
    "import numpy as np\n",
    "from utile import parse_param\n",
    "from nn_utile import AUVTraj, AUVStep, AUVRNNDeltaV\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18212/1297660278.py:11: UserWarning: GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower than expected.\n",
      "  warnings.warn(\n",
      "[2023-05-03 17:25:41,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-05-03 17:25:41,934] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_18212/1297660278.py:22\n",
      "[2023-05-03 17:25:41,934] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-05-03 17:25:41,935] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR nn [TorchVariable(<module 'torch' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py'>)]\n",
      "[2023-05-03 17:25:41,935] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR functional [TorchVariable(<module 'torch.nn' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/__init__.py'>)]\n",
      "[2023-05-03 17:25:41,936] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR relu [TorchVariable(<module 'torch.nn.functional' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/functional.py'>)]\n",
      "[2023-05-03 17:25:41,937] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TorchVariable(<function relu at 0x7f0701008e50>)]\n",
      "[2023-05-03 17:25:41,937] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR lin [TorchVariable(<function relu at 0x7f0701008e50>), NNModuleVariable()]\n",
      "[2023-05-03 17:25:41,939] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [TorchVariable(<function relu at 0x7f0701008e50>), NNModuleVariable()]\n",
      "[2023-05-03 17:25:41,939] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TorchVariable(<function relu at 0x7f0701008e50>), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 17:25:41,950] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TorchVariable(<function relu at 0x7f0701008e50>), TensorVariable()]\n",
      "[2023-05-03 17:25:41,955] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-05-03 17:25:41,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-05-03 17:25:41,956] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-05-03 17:25:41,956] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_18212/1297660278.py, line 22 in forward>])\n",
      "[2023-05-03 17:25:41,958] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: Error detected in ReluBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/tmp/ipykernel_18212/1297660278.py\", line 22, in forward\n",
      "    return torch.nn.functional.relu(self.lin(x))\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataDependentOutputException\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2087\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2087\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2088\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m         _autowrap_check(\n\u001b[1;32m    773\u001b[0m             patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m         {},\n\u001b[1;32m    780\u001b[0m         type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    651\u001b[0m tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1156\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1156\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1108\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1109\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1076\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1076\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1077\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1048\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1048\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1049\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1017\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mwith\u001b[39;00m fx_traceback\u001b[39m.\u001b[39mpreserve_node_meta():\n\u001b[0;32m-> 1017\u001b[0m         backward_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[1;32m   1018\u001b[0m             needed_outs,\n\u001b[1;32m   1019\u001b[0m             grad_primals,\n\u001b[1;32m   1020\u001b[0m             grad_outputs\u001b[39m=\u001b[39;49mneeded_tangents,\n\u001b[1;32m   1021\u001b[0m             allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m backward_out_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(backward_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m     result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:487\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym_mode\u001b[39m.\u001b[39menable(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_torch_dispatch(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:512\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.inner_torch_dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 512\u001b[0m out \u001b[39m=\u001b[39m proxy_call(\u001b[39mself\u001b[39;49m, func, args, kwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:345\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, args, kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mproxy \u001b[39m=\u001b[39m proxy_out\n\u001b[0;32m--> 345\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[39m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# live on CPU, but we could.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1162\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m run_impl_check(func):\n\u001b[0;32m-> 1162\u001b[0m     op_impl_out \u001b[39m=\u001b[39m op_impl(\u001b[39mself\u001b[39;49m, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m op_impl_out \u001b[39m!=\u001b[39m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:410\u001b[0m, in \u001b[0;36mlocal_scalar_dense\u001b[0;34m(fake_mode, func, arg)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m fake_mode\u001b[39m.\u001b[39mshape_env \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Without symints/symfloats, cannot handle this\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m DataDependentOutputException(func)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m is_float_dtype(arg\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mDataDependentOutputException\u001b[0m: aten._local_scalar_dense.default",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m mod \u001b[39m=\u001b[39m MyModule()\n\u001b[1;32m     25\u001b[0m opt_mod \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(mod)\n\u001b[0;32m---> 26\u001b[0m \u001b[39mprint\u001b[39m(opt_mod(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m10\u001b[39;49m, \u001b[39m100\u001b[39;49m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "gpu_ok = False\n",
    "if torch.cuda.is_available():\n",
    "    device_cap = torch.cuda.get_device_capability()\n",
    "    if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "        gpu_ok = True\n",
    "\n",
    "if not gpu_ok:\n",
    "    warnings.warn(\n",
    "        \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower \"\n",
    "        \"than expected.\"\n",
    "    )\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "mod = MyModule()\n",
    "opt_mod = torch.compile(mod)\n",
    "print(opt_mod(torch.randn(10, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "import logging\n",
    "torch._dynamo.logging.set_loggers_level(logging.DEBUG)\n",
    "\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, in_size=10, out_size=10):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.fc0 = torch.nn.Linear(in_size, 100)\n",
    "        self.fc1 = torch.nn.Linear(100, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc0 = self.fc0(x)\n",
    "        return fc0\n",
    "\n",
    "size_in, size_out=100, 100\n",
    "m = MyModule(size_in, size_out)\n",
    "\n",
    "compiled_m = torch.compile(m)\n",
    "\n",
    "\n",
    "a = torch.rand(100, size_in).to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-03 17:21:17,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-05-03 17:21:17,799] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_18212/4199708273.py:19\n",
      "[2023-05-03 17:21:17,799] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-05-03 17:21:17,800] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR fc0 [NNModuleVariable()]\n",
      "[2023-05-03 17:21:17,802] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [NNModuleVariable()]\n",
      "[2023-05-03 17:21:17,802] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [NNModuleVariable(), TensorVariable()]\n"
     ]
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "\n\nfrom user code:\n   File \"/tmp/ipykernel_18212/4199708273.py\", line 19, in forward\n    fc0 = self.fc0(x)\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:1199\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(output_graph, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39massert\u001b[39;00m nnmodule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m     \u001b[39mreturn\u001b[39;00m nnmodule(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1200\u001b[0m \u001b[39melif\u001b[39;00m op \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mget_attr\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1140\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m-> 1140\u001b[0m         \u001b[39mreturn\u001b[39;00m decomposition_table[func](\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m   1143\u001b[0m     \u001b[39m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_prims_common/wrappers.py:220\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             kwargs[k] \u001b[39m=\u001b[39m out_attr\n\u001b[0;32m--> 220\u001b[0m result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    221\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m     \u001b[39misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    223\u001b[0m     \u001b[39mand\u001b[39;00m is_tensor\n\u001b[1;32m    224\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Tuple)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_names)\n\u001b[1;32m    226\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_decomp/decompositions.py:70\u001b[0m, in \u001b[0;36mtype_casts.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 70\u001b[0m r \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtree_map(increase_prec, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtree_map(increase_prec, kwargs))\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m compute_dtype_only:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_decomp/decompositions.py:1126\u001b[0m, in \u001b[0;36maddmm\u001b[0;34m(self, mat1, mat2, beta, alpha)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(alpha)\n\u001b[0;32m-> 1126\u001b[0m out \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mmm(mat1, mat2)\n\u001b[1;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1177\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[39mreturn\u001b[39;00m run_fallback_kernel(\u001b[39mself\u001b[39m, func, args, kwargs, not_implemented_error)\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrap_meta_outputs_with_default_device_logic(r, func, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1233\u001b[0m, in \u001b[0;36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic\u001b[0;34m(self, r, func, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_map(partial(wrap, device\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m]), r)\n\u001b[0;32m-> 1233\u001b[0m \u001b[39mreturn\u001b[39;00m tree_map(partial(wrap), r)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1254\u001b[0m, in \u001b[0;36mFakeTensorMode.gen_wrap_fn.<locals>.wrap\u001b[0;34m(e, device)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m common_device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1251\u001b[0m     (\n\u001b[1;32m   1252\u001b[0m         common_device,\n\u001b[1;32m   1253\u001b[0m         has_scalar_only_inputs,\n\u001b[0;32m-> 1254\u001b[0m     ) \u001b[39m=\u001b[39m FakeTensor\u001b[39m.\u001b[39;49m_find_common_device(func, args, kwargs)\n\u001b[1;32m   1256\u001b[0m \u001b[39mif\u001b[39;00m has_scalar_only_inputs:\n\u001b[1;32m   1257\u001b[0m     \u001b[39m# Under FakeTensorMode, op accepts scalar only inputs, such as aten.add/sub/mul/div,\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m     \u001b[39m# returns a real scalar tensor on CPU. See TensorMeta() in _prims/__init__.py for details.\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m     \u001b[39m# We thus directly convert real tensor to fake tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:924\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    921\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m, found two different devices \u001b[39m\u001b[39m{\u001b[39;00mcommon_device\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m     )\n\u001b[0;32m--> 924\u001b[0m tree_map(merge_devices, args)\n\u001b[1;32m    925\u001b[0m tree_map(merge_devices, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:920\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device.<locals>.merge_devices\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39m# mismatching devices of non-zero dim tensors, throw\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m# This might be valid behavior and need to be explicitly modeled, e.g. reshape_as\u001b[39;00m\n\u001b[0;32m--> 920\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    921\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m, found two different devices \u001b[39m\u001b[39m{\u001b[39;00mcommon_device\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unhandled FakeTensor Device Propagation for aten.mm.default, found two different devices cuda:0, cpu",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:1152\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwith\u001b[39;00m tx\u001b[39m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 1152\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap_fake_exception(\n\u001b[1;32m   1153\u001b[0m             \u001b[39mlambda\u001b[39;49;00m: run_node(tx\u001b[39m.\u001b[39;49moutput, node, args, kwargs, nnmodule)\n\u001b[1;32m   1154\u001b[0m         )\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:808\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 808\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[1;32m    809\u001b[0m \u001b[39mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:1153\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwith\u001b[39;00m tx\u001b[39m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   1152\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap_fake_exception(\n\u001b[0;32m-> 1153\u001b[0m             \u001b[39mlambda\u001b[39;00m: run_node(tx\u001b[39m.\u001b[39;49moutput, node, args, kwargs, nnmodule)\n\u001b[1;32m   1154\u001b[0m         )\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:1206\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(output_graph, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed running \u001b[39m\u001b[39m{\u001b[39;00mop\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mtarget\u001b[39m}\u001b[39;00m\u001b[39m(*\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, **\u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m(scroll up for backtrace)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(op)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed running call_module self_fc0(*(FakeTensor(FakeTensor(..., device='meta', size=(100, 100)), cuda:0),), **{}):\nUnhandled FakeTensor Device Propagation for aten.mm.default, found two different devices cuda:0, cpu\n(scroll up for backtrace)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     compiled_m(a)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:342\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    340\u001b[0m reason \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_fn(\u001b[39mself\u001b[39;49m, inst)\n\u001b[1;32m    343\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported \u001b[39mas\u001b[39;00m excp:\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_backedge() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_compile_partial_graph():\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:965\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    963\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopn(inst\u001b[39m.\u001b[39margval)\n\u001b[1;32m    964\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop()\n\u001b[0;32m--> 965\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_function(fn, args, {})\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:474\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(kwargs, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    470\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    471\u001b[0m     \u001b[39misinstance\u001b[39m(x, VariableTracker)\n\u001b[1;32m    472\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(args, kwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    473\u001b[0m )\n\u001b[0;32m--> 474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush(fn\u001b[39m.\u001b[39;49mcall_function(\u001b[39mself\u001b[39;49m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/nn_module.py:203\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_type \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mcls_to_become\n\u001b[1;32m    201\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m wrap_fx_proxy\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap_fx_proxy(\n\u001b[1;32m    204\u001b[0m         tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m    205\u001b[0m         proxy\u001b[39m=\u001b[39;49mtx\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcreate_proxy(\n\u001b[1;32m    206\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcall_module\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_key,\n\u001b[1;32m    208\u001b[0m             \u001b[39m*\u001b[39;49mproxy_args_kwargs(args, kwargs),\n\u001b[1;32m    209\u001b[0m         ),\n\u001b[1;32m    210\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[39m# for lazy modules, run the pre-hooks which will update the type\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# TODO mlazos: we don't fully support all of the hooks that exist,\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# so restrict using __call__ only to lazy modules for now\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource, (\n\u001b[1;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust provide a valid source in order to inline, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msince inlined function may have default args which must be guarded.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/builder.py:754\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, **options)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_fx_proxy\u001b[39m(tx, proxy, example_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m--> 754\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap_fx_proxy_cls(\n\u001b[1;32m    755\u001b[0m         target_cls\u001b[39m=\u001b[39;49mTensorVariable,\n\u001b[1;32m    756\u001b[0m         tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m    757\u001b[0m         proxy\u001b[39m=\u001b[39;49mproxy,\n\u001b[1;32m    758\u001b[0m         example_value\u001b[39m=\u001b[39;49mexample_value,\n\u001b[1;32m    759\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions,\n\u001b[1;32m    760\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/builder.py:789\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, ignore_subclass, **options)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mwith\u001b[39;00m preserve_rng_state():\n\u001b[1;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m example_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 789\u001b[0m         example_value \u001b[39m=\u001b[39m get_fake_value(proxy\u001b[39m.\u001b[39;49mnode, tx)\n\u001b[1;32m    791\u001b[0m     \u001b[39m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(example_value, FakeTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:1173\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1170\u001b[0m     cause, torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39msymbolic_shapes\u001b[39m.\u001b[39mGuardOnDataDependentSymNode\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m     unimplemented(\u001b[39m\"\u001b[39m\u001b[39mguard on data-dependent symbolic int/float\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1173\u001b[0m \u001b[39mraise\u001b[39;00m TorchRuntimeError() \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m: \n\nfrom user code:\n   File \"/tmp/ipykernel_18212/4199708273.py\", line 19, in forward\n    fc0 = self.fc0(x)\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    compiled_m(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(gpu=False, unit=0):\n",
    "    use_cuda = False\n",
    "    if gpu:\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if not use_cuda:\n",
    "            warnings.warn(\"Asked for GPU but torch couldn't find a Cuda capable device\")\n",
    "    return torch.device(f\"cuda:{unit}\" if use_cuda else \"cpu\")\n",
    "\n",
    "'''\n",
    "Runs a given RNN Model with a given input state and action sequence.\n",
    "'''\n",
    "def run(model, state, X):\n",
    "    # TODO: disable all log, just keep the trajectory.\n",
    "    traj = model(state, X)\n",
    "    return traj\n",
    "\n",
    "'''\n",
    "Load a RNN Model given a checkpoint file.\n",
    "'''\n",
    "def load_model(model, ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    model.load_state_dict(ckpt)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(params, device):\n",
    "    model = AUVTraj(params).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../train_log/2023.04.24-11:25:38/\"\n",
    "param_file = os.path.join(path, \"parameters.yaml\")\n",
    "param = parse_param(param_file)\n",
    "ckpt_path = os.path.join(path, \"ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(True)\n",
    "\n",
    "state = torch.zeros(size=(20, 1, 13)).to(device)\n",
    "state[..., 6] = 1.\n",
    "seq = torch.zeros(size=(20, 10, 6)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-03 15:53:07,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-05-03 15:53:07,255] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-05-03 15:53:07,256] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 24 [TensorVariable()]\n",
      "[2023-05-03 15:53:07,256] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR flatten [TensorVariable()]\n",
      "[2023-05-03 15:53:07,257] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST -2 [GetAttrVariable(TensorVariable(), flatten)]\n",
      "[2023-05-03 15:53:07,258] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST ('start_dim',) [GetAttrVariable(TensorVariable(), flatten), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,259] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION_KW 1 [GetAttrVariable(TensorVariable(), flatten), ConstantVariable(int), ConstantVariable(tuple)]\n",
      "[2023-05-03 15:53:07,262] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST r [TensorVariable()]\n",
      "[2023-05-03 15:53:07,263] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/workspace/rnn-tests/scripts/nn_utile.py:167\n",
      "[2023-05-03 15:53:07,264] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-05-03 15:53:07,264] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR concat [TorchVariable(<module 'torch' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py'>)]\n",
      "[2023-05-03 15:53:07,265] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST r [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>)]\n",
      "[2023-05-03 15:53:07,265] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST v [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), TensorVariable()]\n",
      "[2023-05-03 15:53:07,266] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST u [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), TensorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,266] torch._dynamo.symbolic_convert: [DEBUG] TRACE BUILD_LIST 3 [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), TensorVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,267] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST -1 [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), ListVariable()]\n",
      "[2023-05-03 15:53:07,267] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST ('dim',) [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), ListVariable(), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,267] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION_KW 2 [TorchVariable(<built-in method concat of type object at 0x7f07c21c5540>), ListVariable(), ConstantVariable(int), ConstantVariable(tuple)]\n",
      "[2023-05-03 15:53:07,270] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input_seq [TensorVariable()]\n",
      "[2023-05-03 15:53:07,271] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/workspace/rnn-tests/scripts/nn_utile.py:169\n",
      "[2023-05-03 15:53:07,271] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST h0 []\n",
      "[2023-05-03 15:53:07,272] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [ConstantVariable(NoneType)]\n",
      "[2023-05-03 15:53:07,272] torch._dynamo.symbolic_convert: [DEBUG] TRACE IS_OP 0 [ConstantVariable(NoneType), ConstantVariable(NoneType)]\n",
      "[2023-05-03 15:53:07,273] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 76 [ConstantVariable(bool)]\n",
      "[2023-05-03 15:53:07,273] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/workspace/rnn-tests/scripts/nn_utile.py:170\n",
      "[2023-05-03 15:53:07,273] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-05-03 15:53:07,274] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR init_hidden [NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,275] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST k [UserMethodVariable(<function AUVRNNDeltaV.init_hidden at 0x7f06d1e50dc0>, NNModuleVariable())]\n",
      "[2023-05-03 15:53:07,276] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [UserMethodVariable(<function AUVRNNDeltaV.init_hidden at 0x7f06d1e50dc0>, NNModuleVariable()), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,276] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR device [UserMethodVariable(<function AUVRNNDeltaV.init_hidden at 0x7f06d1e50dc0>, NNModuleVariable()), ConstantVariable(int), UserDefinedObjectVariable(LieTensor)]\n",
      "[2023-05-03 15:53:07,277] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [UserMethodVariable(<function AUVRNNDeltaV.init_hidden at 0x7f06d1e50dc0>, NNModuleVariable()), ConstantVariable(int), GetAttrVariable(UserDefinedObjectVariable(LieTensor), device)]\n",
      "[2023-05-03 15:53:07,278] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object init_hidden at 0x7f06d27112f0, file \"/home/pierre/workspace/rnn-tests/scripts/nn_utile.py\", line 188> \n",
      " 189           0 LOAD_GLOBAL              0 (torch)\n",
      "              2 LOAD_METHOD              1 (zeros)\n",
      "              4 LOAD_FAST                0 (self)\n",
      "              6 LOAD_ATTR                2 (rnn_layers)\n",
      "              8 LOAD_FAST                1 (k)\n",
      "             10 LOAD_FAST                0 (self)\n",
      "             12 LOAD_ATTR                3 (rnn_hidden_size)\n",
      "             14 CALL_METHOD              3\n",
      "             16 LOAD_METHOD              4 (to)\n",
      "             18 LOAD_FAST                2 (device)\n",
      "             20 CALL_METHOD              1\n",
      "             22 RETURN_VALUE\n",
      " \n",
      "\n",
      "[2023-05-03 15:53:07,279] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/workspace/rnn-tests/scripts/nn_utile.py:189\n",
      "[2023-05-03 15:53:07,279] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-05-03 15:53:07,279] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR zeros [TorchVariable(<module 'torch' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py'>)]\n",
      "[2023-05-03 15:53:07,280] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>)]\n",
      "[2023-05-03 15:53:07,280] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR rnn_layers [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,281] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST k [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,282] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,282] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR rnn_hidden_size [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,283] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 3 [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,286] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR to [TensorVariable()]\n",
      "[2023-05-03 15:53:07,287] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST device [GetAttrVariable(TensorVariable(), to)]\n",
      "[2023-05-03 15:53:07,287] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [GetAttrVariable(TensorVariable(), to), GetAttrVariable(UserDefinedObjectVariable(LieTensor), device)]\n",
      "[2023-05-03 15:53:07,287] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object init_hidden at 0x7f06d27112f0, file \"/home/pierre/workspace/rnn-tests/scripts/nn_utile.py\", line 188>\n",
      "[2023-05-03 15:53:07,288] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 1 nodes\n",
      "[2023-05-03 15:53:07,288] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 430, in proxy_args_kwargs\n",
      "    proxy_args = tuple(arg.as_proxy() for arg in args)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 430, in <genexpr>\n",
      "    proxy_args = tuple(arg.as_proxy() for arg in args)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/misc.py\", line 660, in as_proxy\n",
      "    return GetAttrVariable.create_getattr_proxy(self.obj.as_proxy(), self.name)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/base.py\", line 206, in as_proxy\n",
      "    raise NotImplementedError(str(self))\n",
      "NotImplementedError: UserDefinedObjectVariable(LieTensor)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 342, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 965, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 474, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/functions.py\", line 291, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/functions.py\", line 259, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/functions.py\", line 92, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 510, in inline_user_function_return\n",
      "    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1806, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1862, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n",
      "    and self.step()\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 342, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 965, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 474, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/misc.py\", line 744, in call_function\n",
      "    return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 429, in call_method\n",
      "    *proxy_args_kwargs([self] + list(args), kwargs),\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 437, in proxy_args_kwargs\n",
      "    raise unimplemented(\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: call_function args: TensorVariable() GetAttrVariable(UserDefinedObjectVariable(LieTensor), device) \n",
      "[2023-05-03 15:53:07,291] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-05-03 15:53:07,292] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function args: TensorVariable() GetAttrVariable(UserDefinedObjectVariable(LieTensor), device) ', user_stack=[<FrameSummary file /home/pierre/workspace/rnn-tests/scripts/nn_utile.py, line 170 in <graph break in forward>>, <FrameSummary file /home/pierre/workspace/rnn-tests/scripts/nn_utile.py, line 189 in init_hidden>])\n",
      "[2023-05-03 15:53:07,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-05-03 15:53:07,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22\n",
      "[2023-05-03 15:53:07,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22\n",
      "[2023-05-03 15:53:07,350] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-05-03 15:53:07,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing init_hidden\n",
      "[2023-05-03 15:53:07,355] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/workspace/rnn-tests/scripts/nn_utile.py:189\n",
      "[2023-05-03 15:53:07,355] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-05-03 15:53:07,356] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR zeros [TorchVariable(<module 'torch' from '/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py'>)]\n",
      "[2023-05-03 15:53:07,356] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>)]\n",
      "[2023-05-03 15:53:07,357] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR rnn_layers [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,358] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST k [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,358] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,358] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR rnn_hidden_size [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,359] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 3 [TorchVariable(<built-in method zeros of type object at 0x7f07c21c5540>), ConstantVariable(int), ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-05-03 15:53:07,361] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR to [TensorVariable()]\n",
      "[2023-05-03 15:53:07,362] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST device [GetAttrVariable(TensorVariable(), to)]\n",
      "[2023-05-03 15:53:07,362] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [GetAttrVariable(TensorVariable(), to), ConstantVariable(device)]\n",
      "[2023-05-03 15:53:07,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-05-03 15:53:07,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing init_hidden (RETURN_VALUE)\n",
      "[2023-05-03 15:53:07,367] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-05-03 15:53:07,367] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /home/pierre/workspace/rnn-tests/scripts/nn_utile.py, line 189 in init_hidden>])\n",
      "[2023-05-03 15:53:07,369] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-05-03 15:53:07,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23\n",
      "[2023-05-03 15:53:07,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23\n",
      "[2023-05-03 15:53:07,391] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-05-03 15:53:07,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-05-03 15:53:07,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-05-03 15:53:07,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE GET_ITER None [NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,401] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,401] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,401] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,402] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,402] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,407] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,407] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,408] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,408] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,409] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,409] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,409] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,410] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,410] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,425] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,426] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,426] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,427] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,427] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,428] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,428] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,428] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,429] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,433] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,433] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,434] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,434] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,434] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,435] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,435] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,436] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,436] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,441] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,441] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,442] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,442] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,443] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,443] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,443] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,444] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,444] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Done\n",
      "Warm-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-03 15:53:07,468] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,469] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,469] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,470] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,470] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,471] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,471] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,472] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,472] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,477] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,478] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,478] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,478] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,479] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST module [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,479] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\n",
      "[2023-05-03 15:53:07,480] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST module [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,480] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input [ListIteratorVariable(), NNModuleVariable()]\n",
      "[2023-05-03 15:53:07,481] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [ListIteratorVariable(), NNModuleVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,487] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST input [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-05-03 15:53:07,487] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 4 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,488] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:216\n",
      "[2023-05-03 15:53:07,488] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 18 [ListIteratorVariable()]\n",
      "[2023-05-03 15:53:07,489] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py:218\n",
      "[2023-05-03 15:53:07,489] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST input []\n",
      "[2023-05-03 15:53:07,490] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-05-03 15:53:07,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-05-03 15:53:07,490] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-05-03 15:53:07,491] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py, line 218 in forward>])\n",
      "[2023-05-03 15:53:07,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataDependentOutputException\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2087\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2087\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2088\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m         _autowrap_check(\n\u001b[1;32m    773\u001b[0m             patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m         {},\n\u001b[1;32m    780\u001b[0m         type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    651\u001b[0m tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1156\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1156\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1108\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1109\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1076\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1076\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1077\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1048\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1048\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1049\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1017\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mwith\u001b[39;00m fx_traceback\u001b[39m.\u001b[39mpreserve_node_meta():\n\u001b[0;32m-> 1017\u001b[0m         backward_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[1;32m   1018\u001b[0m             needed_outs,\n\u001b[1;32m   1019\u001b[0m             grad_primals,\n\u001b[1;32m   1020\u001b[0m             grad_outputs\u001b[39m=\u001b[39;49mneeded_tangents,\n\u001b[1;32m   1021\u001b[0m             allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m backward_out_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(backward_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m     result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:487\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym_mode\u001b[39m.\u001b[39menable(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_torch_dispatch(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:512\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.inner_torch_dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 512\u001b[0m out \u001b[39m=\u001b[39m proxy_call(\u001b[39mself\u001b[39;49m, func, args, kwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:345\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, args, kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mproxy \u001b[39m=\u001b[39m proxy_out\n\u001b[0;32m--> 345\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[39m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# live on CPU, but we could.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1162\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m run_impl_check(func):\n\u001b[0;32m-> 1162\u001b[0m     op_impl_out \u001b[39m=\u001b[39m op_impl(\u001b[39mself\u001b[39;49m, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m op_impl_out \u001b[39m!=\u001b[39m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:410\u001b[0m, in \u001b[0;36mlocal_scalar_dense\u001b[0;34m(fake_mode, func, arg)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m fake_mode\u001b[39m.\u001b[39mshape_env \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Without symints/symfloats, cannot handle this\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m DataDependentOutputException(func)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m is_float_dtype(arg\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mDataDependentOutputException\u001b[0m: aten._local_scalar_dense.default",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWarm-start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m res, h \u001b[39m=\u001b[39m compiled_dv(x, v, seq[:, \u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m], h)\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m profile(with_stack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, profile_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m prof:\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m record_function(\u001b[39m\"\u001b[39m\u001b[39mmodel_inference\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:165\u001b[0m, in \u001b[0;36mAUVRNNDeltaV.forward\u001b[0;34m(self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:170\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    167\u001b[0m input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[1;32m    173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:173\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, input_seq)\u001b[0m\n\u001b[1;32m    170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[0;32m--> 173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m dv[:, \u001b[39mNone\u001b[39;00m], hN\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dv = AUVRNNDeltaV().to(device)\n",
    "h = None\n",
    "p = state[..., :7]\n",
    "v = state[..., 7:]\n",
    "\n",
    "x = pp.SE3(p).to(device)\n",
    "# Warm-up\n",
    "print(\"Compiling...\")\n",
    "compiled_dv = torch.compile(dv)\n",
    "torch.no_grad()\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "print(\"Warm-start\")\n",
    "res, h = compiled_dv(x, v, seq[:, 0:1], h)\n",
    "\n",
    "with profile(with_stack=True, profile_memory=True, use_cuda=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        res, h = compiled_dv(x, v, seq[:, 0:1], h)\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n=10).table(sort_by=\"self_cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataDependentOutputException\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2087\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2087\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2088\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m         _autowrap_check(\n\u001b[1;32m    773\u001b[0m             patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m         {},\n\u001b[1;32m    780\u001b[0m         type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    651\u001b[0m tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1156\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1156\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1108\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1109\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1076\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1076\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1077\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1048\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1048\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1049\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1017\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mwith\u001b[39;00m fx_traceback\u001b[39m.\u001b[39mpreserve_node_meta():\n\u001b[0;32m-> 1017\u001b[0m         backward_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[1;32m   1018\u001b[0m             needed_outs,\n\u001b[1;32m   1019\u001b[0m             grad_primals,\n\u001b[1;32m   1020\u001b[0m             grad_outputs\u001b[39m=\u001b[39;49mneeded_tangents,\n\u001b[1;32m   1021\u001b[0m             allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m backward_out_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(backward_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m     result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:487\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym_mode\u001b[39m.\u001b[39menable(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_torch_dispatch(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:512\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.inner_torch_dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 512\u001b[0m out \u001b[39m=\u001b[39m proxy_call(\u001b[39mself\u001b[39;49m, func, args, kwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:345\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, args, kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mproxy \u001b[39m=\u001b[39m proxy_out\n\u001b[0;32m--> 345\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[39m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# live on CPU, but we could.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1162\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m run_impl_check(func):\n\u001b[0;32m-> 1162\u001b[0m     op_impl_out \u001b[39m=\u001b[39m op_impl(\u001b[39mself\u001b[39;49m, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m op_impl_out \u001b[39m!=\u001b[39m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:410\u001b[0m, in \u001b[0;36mlocal_scalar_dense\u001b[0;34m(fake_mode, func, arg)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m fake_mode\u001b[39m.\u001b[39mshape_env \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Without symints/symfloats, cannot handle this\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m DataDependentOutputException(func)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m is_float_dtype(arg\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mDataDependentOutputException\u001b[0m: aten._local_scalar_dense.default",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Warm-up\u001b[39;00m\n\u001b[1;32m      9\u001b[0m compiled_step \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(step)\n\u001b[0;32m---> 10\u001b[0m x, v, dv, h \u001b[39m=\u001b[39m compiled_step(x, v, seq[:, \u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m], h)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m profile(with_stack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, profile_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m prof:\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m record_function(\u001b[39m\"\u001b[39m\u001b[39mmodel_inference\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:257\u001b[0m, in \u001b[0;36mAUVStep.forward\u001b[0;34m(self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 257\u001b[0m     dv, h_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv_pred(x, v, u, h0)\n\u001b[1;32m    259\u001b[0m     dv_unnormed \u001b[39m=\u001b[39m dv\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean\n\u001b[1;32m    261\u001b[0m     t \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mse3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt\u001b[39m*\u001b[39mv)\u001b[39m.\u001b[39mExp()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:165\u001b[0m, in \u001b[0;36mAUVRNNDeltaV.forward\u001b[0;34m(self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:170\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    167\u001b[0m input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[1;32m    173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:173\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, input_seq)\u001b[0m\n\u001b[1;32m    170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[0;32m--> 173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m dv[:, \u001b[39mNone\u001b[39;00m], hN\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "step = AUVStep().to(device)\n",
    "h = None\n",
    "p = state[..., :7]\n",
    "v = state[..., 7:]\n",
    "\n",
    "x = pp.SE3(p).to(device)\n",
    "\n",
    "# Warm-up\n",
    "print(\"Compiling...\")\n",
    "compiled_step = torch.compile(step)\n",
    "print(\"Done\")\n",
    "x, v, dv, h = compiled_step(x, v, seq[:, 0:1], h)\n",
    "\n",
    "with profile(with_stack=True, profile_memory=True, use_cuda=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        compiled_step(x, v, seq[:, 0:1], h)\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n=10).table(sort_by=\"self_cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/pierre/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataDependentOutputException\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2087\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2087\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2088\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m         _autowrap_check(\n\u001b[1;32m    773\u001b[0m             patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m         {},\n\u001b[1;32m    780\u001b[0m         type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    651\u001b[0m tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1156\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1156\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1108\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1109\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1076\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1076\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1077\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1048\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1048\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1049\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1017\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mwith\u001b[39;00m fx_traceback\u001b[39m.\u001b[39mpreserve_node_meta():\n\u001b[0;32m-> 1017\u001b[0m         backward_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[1;32m   1018\u001b[0m             needed_outs,\n\u001b[1;32m   1019\u001b[0m             grad_primals,\n\u001b[1;32m   1020\u001b[0m             grad_outputs\u001b[39m=\u001b[39;49mneeded_tangents,\n\u001b[1;32m   1021\u001b[0m             allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m backward_out_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(backward_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m     result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:487\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym_mode\u001b[39m.\u001b[39menable(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_torch_dispatch(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:512\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.inner_torch_dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 512\u001b[0m out \u001b[39m=\u001b[39m proxy_call(\u001b[39mself\u001b[39;49m, func, args, kwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:345\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, args, kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mproxy \u001b[39m=\u001b[39m proxy_out\n\u001b[0;32m--> 345\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[39m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# live on CPU, but we could.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:1162\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m run_impl_check(func):\n\u001b[0;32m-> 1162\u001b[0m     op_impl_out \u001b[39m=\u001b[39m op_impl(\u001b[39mself\u001b[39;49m, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m op_impl_out \u001b[39m!=\u001b[39m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py:410\u001b[0m, in \u001b[0;36mlocal_scalar_dense\u001b[0;34m(fake_mode, func, arg)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m fake_mode\u001b[39m.\u001b[39mshape_env \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Without symints/symfloats, cannot handle this\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m DataDependentOutputException(func)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m is_float_dtype(arg\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mDataDependentOutputException\u001b[0m: aten._local_scalar_dense.default",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Warm-up\u001b[39;00m\n\u001b[1;32m      4\u001b[0m compiled_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(model)\n\u001b[0;32m----> 5\u001b[0m traj \u001b[39m=\u001b[39m compiled_model(state, seq)\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m profile(with_stack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, profile_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m prof:\n\u001b[1;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m record_function(\u001b[39m\"\u001b[39m\u001b[39mmodel_inference\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:342\u001b[0m, in \u001b[0;36mAUVTraj.forward\u001b[0;34m(self, x, U)\u001b[0m\n\u001b[1;32m    340\u001b[0m traj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m7\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    341\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mse3:\n\u001b[0;32m--> 342\u001b[0m     traj \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mSE3(traj)\n\u001b[1;32m    343\u001b[0m traj_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    344\u001b[0m traj_dv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:346\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, U, k, tau, h, p, v)\u001b[0m\n\u001b[1;32m    343\u001b[0m traj_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    344\u001b[0m traj_dv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 346\u001b[0m x \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mSE3(p)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tau):\n\u001b[1;32m    348\u001b[0m     x_next, v_next, dv, h_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(x, v, U[:, i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], h)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:346\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, U, tau, h, p, v, traj, traj_v, traj_dv)\u001b[0m\n\u001b[1;32m    343\u001b[0m traj_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    344\u001b[0m traj_dv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size\u001b[39m=\u001b[39m(k, tau, \u001b[39m6\u001b[39m))\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 346\u001b[0m x \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mSE3(p)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tau):\n\u001b[1;32m    348\u001b[0m     x_next, v_next, dv, h_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(x, v, U[:, i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], h)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:348\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, U, tau, h, v, traj, traj_v, traj_dv)\u001b[0m\n\u001b[1;32m    346\u001b[0m x \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mSE3(p)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tau):\n\u001b[0;32m--> 348\u001b[0m     x_next, v_next, dv, h_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(x, v, U[:, i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], h)\n\u001b[1;32m    350\u001b[0m     x, v, h \u001b[39m=\u001b[39m x_next, v_next, h_next\n\u001b[1;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mse3:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:257\u001b[0m, in \u001b[0;36mAUVStep.forward\u001b[0;34m(self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 257\u001b[0m     dv, h_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv_pred(x, v, u, h0)\n\u001b[1;32m    259\u001b[0m     dv_unnormed \u001b[39m=\u001b[39m dv\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean\n\u001b[1;32m    261\u001b[0m     t \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mse3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt\u001b[39m*\u001b[39mv)\u001b[39m.\u001b[39mExp()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:165\u001b[0m, in \u001b[0;36mAUVRNNDeltaV.forward\u001b[0;34m(self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:166\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, v, u, h0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     k \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrotation()\u001b[39m.\u001b[39mmatrix()\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    167\u001b[0m     input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:170\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, x, v, u, h0, k)\u001b[0m\n\u001b[1;32m    167\u001b[0m input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([r, v, u], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m h0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[1;32m    173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/rnn-tests/scripts/nn_utile.py:173\u001b[0m, in \u001b[0;36m<graph break in forward>\u001b[0;34m(___stack0, self, input_seq)\u001b[0m\n\u001b[1;32m    170\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(k, x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    172\u001b[0m out, hN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(input_seq, h0)\n\u001b[0;32m--> 173\u001b[0m dv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m dv[:, \u001b[39mNone\u001b[39;00m], hN\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pp_local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised DataDependentOutputException: aten._local_scalar_dense.default\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "model = create_model(param, device)\n",
    "model = load_model(model, ckpt_path)\n",
    "# Warm-up\n",
    "compiled_model = torch.compile(model)\n",
    "traj = compiled_model(state, seq)\n",
    "\n",
    "with profile(with_stack=True, profile_memory=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        compiled_model(state, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
